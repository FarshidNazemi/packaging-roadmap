{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219c91c3-89ee-445c-986e-db8eb745b472",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. General Problem Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d360e3fb-25ea-481f-9ad7-a328e0ab8a85",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8528e657-f6aa-4367-b3df-92d22f5e9cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb89a1d0-4385-4a8a-8125-8001faad6a29",
   "metadata": {},
   "source": [
    "## Import LCA matrices from OpenLCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317cbd1c-cf83-4c5c-b020-f888f300682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_df = pd.read_csv(\"data/A.csv\", header=None)\n",
    "B_df = pd.read_csv(\"data/B.csv\", header=None)\n",
    "C_df = pd.read_csv(\"data/C.csv\", header=None)\n",
    "\n",
    "# Convert all string-looking numbers to floats\n",
    "A = A_df.apply(pd.to_numeric, errors='coerce').values\n",
    "B = B_df.apply(pd.to_numeric, errors='coerce').values\n",
    "C = C_df.apply(pd.to_numeric, errors='coerce').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a29d4e-491b-4db8-8940-f27b33fc2432",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_index_df = pd.read_csv(\"data/index_A.csv\")\n",
    "B_index_df = pd.read_csv(\"data/index_B.csv\")\n",
    "C_index_df = pd.read_csv(\"data/index_C.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd3c91c-c688-4845-9dd3-0047aa3c84bc",
   "metadata": {},
   "source": [
    "## Removing Transportation (deregionalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0c573d-f4af-410f-b4b8-73eb44593f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_transport_df = pd.read_csv(\"data/Transportation_A.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14477a7b-c1c3-4db9-a17a-d6fd552317c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict mapping each provider name to all its indices in A_index_df\n",
    "mapping = A_index_df.groupby('provider name')['index'].apply(list)\n",
    "\n",
    "# build a single flat list of all matching indices for the foreground processes\n",
    "matched_indices_transport = [\n",
    "    idx\n",
    "    for name in A_transport_df['provider name']\n",
    "    if name in mapping\n",
    "    for idx in mapping[name]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3185e174-16e8-4be7-b1e4-bb93bfdba5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# matched_indices_transport is the list of indices to remove\n",
    "to_drop = np.array(sorted(set(matched_indices_transport), key=int))\n",
    "\n",
    "# 1) Remove from A_index_df\n",
    "mask_keep = ~A_index_df['index'].isin(to_drop)\n",
    "A_index_df = A_index_df.loc[mask_keep].copy()\n",
    "\n",
    "# 2) Remove corresponding rows and columns from A\n",
    "A = np.delete(A, to_drop, axis=0)  # remove rows\n",
    "A = np.delete(A, to_drop, axis=1)  # remove columns\n",
    "\n",
    "# 3) Remove the same columns from B (keep rows)\n",
    "B = np.delete(B, to_drop, axis=1)\n",
    "\n",
    "# 4) Reset the index column in A_index_df\n",
    "A_index_df['index'] = np.arange(len(A_index_df), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c9543-2d28-4c36-9e6f-02c4f9afdf8c",
   "metadata": {},
   "source": [
    "## Aggregating electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d9677d-e30f-49fd-9592-85e39589f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_elec_df = pd.read_csv(\"data/Electricity_A.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1243f655-d9ab-449e-b82c-804562147bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs assumed:\n",
    "# A : numeric numpy array (rows x cols)\n",
    "# A_index_df : DataFrame with columns [\"index\", \"provider name\", \"flow name\", ...]\n",
    "# A_elec_df : DataFrame with column [\"provider name\"] listing all electricity providers\n",
    "# The indices in A_index_df[\"index\"] align with both row and column positions of A.\n",
    "\n",
    "# 0) Build the set of electricity provider names\n",
    "elec_names = set(A_elec_df['provider name'].dropna().astype(str).unique())\n",
    "\n",
    "# 1) Find their indices in A_index_df\n",
    "elec_idx = A_index_df.loc[A_index_df['provider name'].astype(str).isin(elec_names), 'index'].astype(int).unique()\n",
    "\n",
    "# 2) Locate the mix row index (must exist)\n",
    "mix_name = \"Electricity Mix (Global)\"\n",
    "mix_rows = A_index_df.loc[A_index_df['provider name'] == mix_name, 'index'].astype(int).unique()\n",
    "if len(mix_rows) == 0:\n",
    "    raise ValueError(\"Electricity Mix (Global) not found in A_index_df['provider name'].\")\n",
    "mix_idx = int(mix_rows[0])\n",
    "\n",
    "# Ensure the mix row is not purged\n",
    "elec_idx_set = set(map(int, elec_idx))\n",
    "elec_idx_wo_mix = sorted(elec_idx_set - {mix_idx})\n",
    "\n",
    "# 3) Aggregate: add all electricity rows (except the mix row) into the mix row, column-wise\n",
    "if len(elec_idx_wo_mix) > 0:\n",
    "    # in case of NaNs\n",
    "    add_block = np.nansum(A[elec_idx_wo_mix, :], axis=0)\n",
    "    A[mix_idx, :] = np.nan_to_num(A[mix_idx, :]) + np.nan_to_num(add_block)\n",
    "\n",
    "# 4) Decide what to drop\n",
    "rows_to_drop = np.array(elec_idx_wo_mix, dtype=int)            # drop electricity rows except the mix row\n",
    "cols_to_drop = np.array(elec_idx_wo_mix, dtype=int)            # drop electricity columns except the mix column\n",
    "\n",
    "# (Optionally also drop the mix COLUMN; keep it if you want to retain that process as a column)\n",
    "# To ALSO drop the mix column, uncomment the next line:\n",
    "# cols_to_drop = np.array(sorted(elec_idx_set), dtype=int)\n",
    "\n",
    "# 5) Remove rows/columns from A and columns from B\n",
    "if rows_to_drop.size > 0:\n",
    "    A = np.delete(A, rows_to_drop, axis=0)\n",
    "if cols_to_drop.size > 0:\n",
    "    A = np.delete(A, cols_to_drop, axis=1)\n",
    "    B = np.delete(B, cols_to_drop, axis=1)\n",
    "\n",
    "# 6) Remove the same rows from A_index_df (only rows; columns in A_index_df are metadata)\n",
    "if len(elec_idx_wo_mix) > 0:\n",
    "    keep_mask = ~A_index_df['index'].astype(int).isin(elec_idx_wo_mix)\n",
    "    A_index_df = A_index_df.loc[keep_mask].copy()\n",
    "\n",
    "# 7) Reset the \"index\" column in A_index_df to reflect 0..n-1 after deletions\n",
    "A_index_df['index'] = np.arange(len(A_index_df), dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002f8f6-b7a2-457b-84fd-122433189943",
   "metadata": {},
   "source": [
    "## Identifying background flows for cost calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6abc3af9-5e07-48e8-80bf-86359dfc5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_foreground_df = pd.read_csv(\"data/Foreground_A.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e88ff9d5-13b0-4b33-9b18-45ea8daeb086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper for consistent comparison\n",
    "norm = lambda s: str(s).strip().casefold()\n",
    "\n",
    "# Normalize keys\n",
    "A_index_df = A_index_df.copy()\n",
    "A_foreground_df = A_foreground_df.copy()\n",
    "A_index_df['provider_key'] = A_index_df['provider name'].map(norm)\n",
    "A_foreground_df['provider_key'] = A_foreground_df['provider name'].map(norm)\n",
    "\n",
    "# Build mapping from provider -> indices\n",
    "idx_map = (\n",
    "    A_index_df\n",
    "    .groupby('provider_key')['index']\n",
    "    .apply(lambda s: list(map(int, s)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Foreground indices (to exclude later)\n",
    "matched_indices = sorted({\n",
    "    idx\n",
    "    for key in A_foreground_df['provider_key'].unique()\n",
    "    for idx in idx_map.get(key, [])\n",
    "})\n",
    "\n",
    "# Example: build all_nonzero from the numeric matrix A\n",
    "# (take all row indices with nonzero entries across the columns in matched_indices)\n",
    "all_nonzero = set()\n",
    "for col in matched_indices:\n",
    "    all_nonzero.update(np.nonzero(A[:, col])[0])\n",
    "\n",
    "# Remove overlap\n",
    "foreground_set = set(matched_indices)\n",
    "all_nonzero_set = {int(x) for x in all_nonzero}\n",
    "filtered_nonzero_rows = sorted(all_nonzero_set - foreground_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4a51dd-0bce-412e-8565-2546b418a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = A_index_df[A_index_df['index'].isin(filtered_nonzero_rows)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca834235-cc09-43db-95ee-bad708c136e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('filtered_nonzero_rows_with_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8cb8fc-75c1-429c-9a9d-60caabad1851",
   "metadata": {},
   "source": [
    "## Importing financial data for cost calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbb1e65-7a1a-481f-8295-48fb400a9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_df = pd.read_csv(\"data/Financial.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be46d0-dd00-4f59-8bc9-def3f14e784a",
   "metadata": {},
   "source": [
    "# 2. Static Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4fa3b-a1dd-4630-99df-bcce5a4250ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1. Base Case (Linear fossil economy in 2025 without carbon capture and microplastic treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ed06f-ce44-49d6-854f-53c5a8298b2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Electricity Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0063c7c-bc37-44ed-b6ed-c335a6b23731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Electricity Mix for 2025\n",
    "electricity_mix_df = pd.read_csv(\"data/electricity_mix.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09efeeff-2c94-4c7c-b4a9-f296bed8dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helper to normalize keys (to avoid case/space mismatches)\n",
    "norm = lambda s: str(s).strip().casefold()\n",
    "\n",
    "# Normalize provider names in both DataFrames\n",
    "A_index_df = A_index_df.copy()\n",
    "electricity_mix_df = electricity_mix_df.copy()\n",
    "A_index_df['provider_key'] = A_index_df['provider name'].map(norm)\n",
    "electricity_mix_df['provider_key'] = electricity_mix_df['provider name'].map(norm)\n",
    "\n",
    "# Find the column index for \"Electricity Mix (Global)\"\n",
    "mix_key = norm(\"Electricity Mix (Global)\")\n",
    "mix_idx_arr = A_index_df.loc[A_index_df['provider_key'] == mix_key, 'index'].astype(int).values\n",
    "if len(mix_idx_arr) == 0:\n",
    "    raise ValueError(\"'Electricity Mix (Global)' not found in A_index_df['provider name']\")\n",
    "mix_col = int(mix_idx_arr[0])\n",
    "\n",
    "# Iterate over providers in electricity_mix_df and assign values into A\n",
    "for _, row in electricity_mix_df.iterrows():\n",
    "    prov_key = row['provider_key']\n",
    "    energy_val = row['2025 Energy Mix']\n",
    "\n",
    "    # find the row index (from A_index_df) for this provider\n",
    "    idxs = A_index_df.loc[A_index_df['provider_key'] == prov_key, 'index'].astype(int).values\n",
    "    for ridx in idxs:\n",
    "        A[ridx, mix_col] = energy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c209732-7f9e-439c-8b57-c85830ade9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1432"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[int(A_index_df.loc[A_index_df[\"provider name\"]==\"Electricity, hydropower (life cycle)\",\"index\"].iloc[0]),\n",
    "  int(A_index_df.loc[A_index_df[\"provider name\"]==\"Electricity Mix (Global)\",\"index\"].iloc[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118e6ab-699d-4d6e-ab32-7fd06fff9c40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Collection and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925efc39-ff11-4ab6-ba19-06ffd143b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "packaging_types_df = pd.read_csv(\"data/packaging_types.csv\")\n",
    "collection_recyclate_df = pd.read_csv(\"data/collection_Recyclate.csv\")\n",
    "collection_msw_df = pd.read_csv(\"data/collection_MSW.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a621c7-06a2-4cdc-9622-bc3702bff381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---- exact-key matcher (case/space insensitive, keeps stage prefixes) ----\n",
    "ekey = lambda s: str(s).strip().casefold()\n",
    "\n",
    "# ---- copies + normalized keys ----\n",
    "A_index_df = A_index_df.copy()\n",
    "packaging_types_df = packaging_types_df.copy()\n",
    "collection_msw_df = collection_msw_df.copy()\n",
    "\n",
    "A_index_df[\"prov_exact\"]         = A_index_df[\"provider name\"].map(ekey)\n",
    "packaging_types_df[\"prov_exact\"] = packaging_types_df[\"provider name\"].map(ekey)\n",
    "collection_msw_df[\"prov_exact\"]  = collection_msw_df[\"provider name\"].map(ekey)\n",
    "\n",
    "# If you know the 3 mapping columns, set them here, e.g.:\n",
    "# mapping_cols = [\"dest_1\", \"dest_2\", \"dest_3\"]\n",
    "# Otherwise, auto-detect up to 3 string columns besides 'provider name'\n",
    "mapping_cols = [c for c in packaging_types_df.columns\n",
    "                if c not in (\"provider name\", \"prov_exact\")\n",
    "                and pd.api.types.is_string_dtype(packaging_types_df[c])][:3]\n",
    "if not mapping_cols:\n",
    "    raise ValueError(\"Couldn't detect mapping columns; please set 'mapping_cols' explicitly.\")\n",
    "\n",
    "# Normalize destination columns\n",
    "for col in mapping_cols:\n",
    "    packaging_types_df[col + \"_exact\"] = packaging_types_df[col].map(ekey)\n",
    "\n",
    "# provider name -> list of integer indices in A (rows/cols)\n",
    "idx_map_exact = (\n",
    "    A_index_df.groupby(\"prov_exact\")[\"index\"]\n",
    "              .apply(lambda s: list(map(int, s)))\n",
    "              .to_dict()\n",
    ")\n",
    "\n",
    "# \"Use & Collection\" column index\n",
    "use_col = int(A_index_df.loc[A_index_df[\"provider name\"]==\"Use & Collection\",\"index\"].iloc[0])\n",
    "\n",
    "# ---- X: abs(A[row, use_col]) per packaging provider (exact match) ----\n",
    "X_by_src = {}\n",
    "for k in packaging_types_df[\"prov_exact\"].dropna().unique():\n",
    "    rows = idx_map_exact.get(k, [])\n",
    "    if rows:\n",
    "        r0 = int(rows[0])  # if multiple rows per provider, change to sum/mean if needed\n",
    "        X_by_src[k] = abs(float(A[r0, use_col]))\n",
    "\n",
    "# ---- Y: MSW \"Linear Economy\" per provider (exact match) ----\n",
    "Y_by_key = (collection_msw_df.dropna(subset=[\"prov_exact\"])\n",
    "            .groupby(\"prov_exact\")[\"Linear Economy\"]\n",
    "            .first()\n",
    "            .to_dict())\n",
    "\n",
    "# ---- zero out destination cells we're going to rewrite (avoid accumulation on reruns) ----\n",
    "dest_rows = set()\n",
    "for _, row in packaging_types_df.iterrows():\n",
    "    for col in mapping_cols:\n",
    "        dk = row.get(col + \"_exact\", None)\n",
    "        if dk in idx_map_exact:\n",
    "            dest_rows.update(idx_map_exact[dk])\n",
    "if dest_rows:\n",
    "    A[list(dest_rows), use_col] = 0.0\n",
    "\n",
    "# ---- assign: A[row, use_col] += X * Y for each mapped destination (exact match) ----\n",
    "for _, row in packaging_types_df.iterrows():\n",
    "    src_k = row[\"prov_exact\"]\n",
    "    X     = X_by_src.get(src_k)\n",
    "    if X is None:\n",
    "        continue\n",
    "    for col in mapping_cols:\n",
    "        dk = row.get(col + \"_exact\", None)\n",
    "        if not dk:\n",
    "            continue\n",
    "        Y = Y_by_key.get(dk)\n",
    "        if Y is None or pd.isna(Y):\n",
    "            continue\n",
    "        contrib = float(abs(X) * float(Y))\n",
    "        for r in idx_map_exact.get(dk, []):\n",
    "            A[int(r), use_col] += contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a637468f-f855-4092-8a77-aaf3bd412721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021840000000000002"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[int(A_index_df.loc[A_index_df[\"provider name\"]==\"Disposal, Multi-material Food Bottles\",\"index\"].iloc[0]),\n",
    "  int(A_index_df.loc[A_index_df[\"provider name\"]==\"Use & Collection\",\"index\"].iloc[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bdc6ddb-8a03-4bfb-8c97-47de0ba52572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Recyclate pass: X * Y_recyclate added into A[:, use_col] ---\n",
    "\n",
    "collection_recyclate_df = collection_recyclate_df.copy()\n",
    "collection_recyclate_df[\"prov_exact\"] = collection_recyclate_df[\"provider name\"].map(ekey)\n",
    "\n",
    "# Y from recyclate\n",
    "Y_rec_by_key = (\n",
    "    collection_recyclate_df.dropna(subset=[\"prov_exact\"])\n",
    "    .groupby(\"prov_exact\")[\"Linear Economy\"]\n",
    "    .first()    # change to sum()/mean() if needed\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Optionally clear destination cells before this pass (default = keep MSW values and add recyclate)\n",
    "reset_dest = False\n",
    "if reset_dest:\n",
    "    rec_dest_rows = set()\n",
    "    for _, row in packaging_types_df.iterrows():\n",
    "        for col in mapping_cols:\n",
    "            dk = row.get(col + \"_exact\", None)\n",
    "            rec_dest_rows.update(idx_map_exact.get(dk, []))\n",
    "    if rec_dest_rows:\n",
    "        A[np.array(sorted(rec_dest_rows), dtype=int), use_col] = 0.0\n",
    "\n",
    "# Write: A[row, use_col] += X * Y_recyclate\n",
    "for _, row in packaging_types_df.iterrows():\n",
    "    src_k = row[\"prov_exact\"]\n",
    "    X = X_by_src.get(src_k)\n",
    "    if X is None:\n",
    "        continue\n",
    "    for col in mapping_cols:\n",
    "        dk = row.get(col + \"_exact\", None)\n",
    "        if not dk:\n",
    "            continue\n",
    "        Y = Y_rec_by_key.get(dk)\n",
    "        if Y is None or pd.isna(Y):\n",
    "            continue\n",
    "        contrib = float(abs(X) * float(Y))\n",
    "        for r in idx_map_exact.get(dk, []):\n",
    "            A[int(r), use_col] += contrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13813ae7-1e2c-4572-9712-cac16fa3a29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[int(A_index_df.loc[A_index_df[\"provider name\"]==\"High-tech Sorting, Collected HDPE Drinking Bottles\",\"index\"].iloc[0]),\n",
    "  int(A_index_df.loc[A_index_df[\"provider name\"]==\"Use & Collection\",\"index\"].iloc[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff7b317-9f95-45f9-8f5d-a198cb125c2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Monomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84907181-084f-423e-afcb-25c5b3b62b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "monomers_decisions_df = pd.read_csv(\"data/monomers_decisions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c70d06a-a354-4162-8d7c-9d0eb9bfe69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- helper: normalize names for reliable matching ----\n",
    "ekey = lambda s: str(s).strip().casefold()\n",
    "\n",
    "# ---- defensive copies + normalized keys ----\n",
    "A_index_df = A_index_df.copy()\n",
    "monomers_decisions_df = monomers_decisions_df.copy()\n",
    "\n",
    "A_index_df[\"prov_key\"] = A_index_df[\"provider name\"].map(ekey)\n",
    "monomers_decisions_df[\"col_key\"]  = monomers_decisions_df[\"Provider name\"].map(ekey)\n",
    "monomers_decisions_df[\"row_key\"]  = monomers_decisions_df[\"Input parameters names\"].map(ekey)\n",
    "\n",
    "# provider key -> list of integer indices in A (rows/cols)\n",
    "idx_map = (\n",
    "    A_index_df.groupby(\"prov_key\")[\"index\"]\n",
    "    .apply(lambda s: list(map(int, s)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# ensure \"Linear Economy\" is numeric\n",
    "monomers_decisions_df[\"Linear Economy\"] = pd.to_numeric(\n",
    "    monomers_decisions_df[\"Linear Economy\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# ---- apply updates: for each row in decisions, scale A[row_idx, col_idx] *= X ----\n",
    "n_pairs = 0\n",
    "skipped = 0\n",
    "\n",
    "for _, drow in monomers_decisions_df.iterrows():\n",
    "    x = drow[\"Linear Economy\"]\n",
    "    if pd.isna(x):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    row_idxs = idx_map.get(drow[\"row_key\"], [])\n",
    "    col_idxs = idx_map.get(drow[\"col_key\"], [])\n",
    "\n",
    "    if not row_idxs or not col_idxs:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    # multiply all combinations (row, col) by X\n",
    "    for ri in row_idxs:\n",
    "        for ci in col_idxs:\n",
    "            A[int(ri), int(ci)] = np.nan_to_num(A[int(ri), int(ci)]) * float(x)\n",
    "            n_pairs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35f4586b-7579-4f0d-9080-45eff0cd55aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[int(A_index_df.loc[A_index_df[\"provider name\"]==\"Ethylene production, fossil, steam cracker\",\"index\"].iloc[0]),\n",
    "  int(A_index_df.loc[A_index_df[\"provider name\"]==\"Ethylene\",\"index\"].iloc[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f2cba-97bc-432b-954c-d2fc62e5ce6d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Carbon Capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec4b673c-8033-40e4-b506-73baaf7ab563",
   "metadata": {},
   "outputs": [],
   "source": [
    "carbon_capture_decisions_df = pd.read_csv(\"data/carbon_capture_decisions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f32d0b6f-787e-4517-9a3d-3ca555550713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# normalize for reliable matching\n",
    "ekey = lambda s: str(s).strip().casefold()\n",
    "\n",
    "# copies + keys\n",
    "A_index_df = A_index_df.copy()\n",
    "carbon_capture_decisions_df = carbon_capture_decisions_df.copy()\n",
    "\n",
    "A_index_df[\"prov_key\"] = A_index_df[\"provider name\"].map(ekey)\n",
    "carbon_capture_decisions_df[\"col_key\"] = carbon_capture_decisions_df[\"Provider name\"].map(ekey)\n",
    "carbon_capture_decisions_df[\"row_key\"] = carbon_capture_decisions_df[\"Input parameters names\"].map(ekey)\n",
    "\n",
    "# provider key -> list of integer indices (row/col) in A\n",
    "idx_map = (\n",
    "    A_index_df.groupby(\"prov_key\")[\"index\"]\n",
    "    .apply(lambda s: list(map(int, s)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# ensure numeric X\n",
    "carbon_capture_decisions_df[\"Linear Economy\"] = pd.to_numeric(\n",
    "    carbon_capture_decisions_df[\"Linear Economy\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# apply: for each decision row, A[row, col] *= X\n",
    "n_pairs = 0\n",
    "skipped = 0\n",
    "\n",
    "for _, d in carbon_capture_decisions_df.iterrows():\n",
    "    X = d[\"Linear Economy\"]\n",
    "    if pd.isna(X):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    row_idxs = idx_map.get(d[\"row_key\"], [])\n",
    "    col_idxs = idx_map.get(d[\"col_key\"], [])\n",
    "\n",
    "    if not row_idxs or not col_idxs:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    for ri in row_idxs:\n",
    "        for ci in col_idxs:\n",
    "            A[int(ri), int(ci)] = np.nan_to_num(A[int(ri), int(ci)]) * float(X)\n",
    "            n_pairs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56328051-5a67-4133-a373-fe67a9f0b092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[int(A_index_df.loc[A_index_df[\"provider name\"]==\"Plastic Incineration CO2 to the atmosphere\",\"index\"].iloc[0]),\n",
    "  int(A_index_df.loc[A_index_df[\"provider name\"]==\"Plastic Incineration CO2\",\"index\"].iloc[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823baeb0-7754-42c0-8dbf-0cef0bc19915",
   "metadata": {
    "tags": []
   },
   "source": [
    "### microplastics treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea48b9fe-e1de-4321-9575-d34e29df59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "microplastics_decisions_df = pd.read_csv(\"data/microplastics_decisions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe476f50-aaa4-4f3d-adb2-027ba7fef0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# normalizer\n",
    "ekey = lambda s: str(s).strip().casefold()\n",
    "\n",
    "# copies + normalized keys\n",
    "A_index_df = A_index_df.copy()\n",
    "microplastics_decisions_df = microplastics_decisions_df.copy()\n",
    "\n",
    "A_index_df[\"prov_key\"] = A_index_df[\"provider name\"].map(ekey)\n",
    "microplastics_decisions_df[\"col_key\"] = microplastics_decisions_df[\"Provider name\"].map(ekey)\n",
    "microplastics_decisions_df[\"row_key\"] = microplastics_decisions_df[\"Input parameters names\"].map(ekey)\n",
    "\n",
    "# provider key -> list of integer indices in A (rows/cols)\n",
    "idx_map = (\n",
    "    A_index_df.groupby(\"prov_key\")[\"index\"]\n",
    "    .apply(lambda s: list(map(int, s)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# ensure numeric decision value\n",
    "microplastics_decisions_df[\"Linear Economy\"] = pd.to_numeric(\n",
    "    microplastics_decisions_df[\"Linear Economy\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# apply decisions: A[row, col] *= value\n",
    "n_pairs = 0\n",
    "skipped = 0\n",
    "\n",
    "for _, d in microplastics_decisions_df.iterrows():\n",
    "    x = d[\"Linear Economy\"]\n",
    "    if pd.isna(x):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    row_idxs = idx_map.get(d[\"row_key\"], [])\n",
    "    col_idxs = idx_map.get(d[\"col_key\"], [])\n",
    "\n",
    "    if not row_idxs or not col_idxs:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    for ri in row_idxs:\n",
    "        for ci in col_idxs:\n",
    "            A[int(ri), int(ci)] = np.nan_to_num(A[int(ri), int(ci)]) * float(x)\n",
    "            n_pairs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a877e1f-36b5-4b50-b0a4-4689f4f41b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[int(A_index_df.loc[A_index_df[\"provider name\"]==\"Plastic Incineration CO2 to the atmosphere\",\"index\"].iloc[0]),\n",
    "  int(A_index_df.loc[A_index_df[\"provider name\"]==\"Plastic Incineration CO2\",\"index\"].iloc[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14109e77-45a0-4afe-9cb2-c36d3aa77845",
   "metadata": {},
   "source": [
    "### LCA/TEA Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2ecbf76-4b18-4106-aa5f-759e5dba89d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.77972205e+00, 8.88848258e-08, 1.04594727e-01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCA Calculations\n",
    "packaging_demand_2025 = 4.29e+11 #429 MMT\n",
    "f = np.zeros(len(A))\n",
    "f[0] = packaging_demand_2025\n",
    "s = np.linalg.solve(A, f)\n",
    "g = B@s\n",
    "phi = C@g\n",
    "phi_normalized = phi/packaging_demand_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd813b01-abe2-4de8-891d-14cab68ea18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 contributors to phi[0]:\n",
      "                                       provider_name  contribution    share_%\n",
      "0                                  Open Burning, PET  3.316361e+10  73.908566\n",
      "1   Open Burning, multi-material food packaging film  2.511470e+09   5.597072\n",
      "2                                   Open Burning, PP  2.024493e+09   4.511792\n",
      "3                                 Open Burning, LDPE  1.438905e+09   3.206750\n",
      "4                                 Open Burning, HDPE  1.357108e+09   3.024455\n",
      "5                                   Open Burning, PS  1.285140e+09   2.864068\n",
      "6          Open Burning, multi-material food bottles  1.212943e+09   2.703169\n",
      "7                                  Open Burning, PVC  9.533400e+08   2.124617\n",
      "8  Open Burning, multi-material non-food packagin...  9.228319e+08   2.056627\n",
      "9                     Electricity, coal (life cycle)  3.717960e+05   0.000829\n",
      "\n",
      "Top 10 contributors to phi[x] (absolute):\n",
      "                                       provider_name  contribution    share_%\n",
      "0                                  Open Burning, PET  3.316361e+10  73.908566\n",
      "1   Open Burning, multi-material food packaging film  2.511470e+09   5.597072\n",
      "2                                   Open Burning, PP  2.024493e+09   4.511792\n",
      "3                                 Open Burning, LDPE  1.438905e+09   3.206750\n",
      "4                                 Open Burning, HDPE  1.357108e+09   3.024455\n",
      "5                                   Open Burning, PS  1.285140e+09   2.864068\n",
      "6          Open Burning, multi-material food bottles  1.212943e+09   2.703169\n",
      "7                                  Open Burning, PVC  9.533400e+08   2.124617\n",
      "8  Open Burning, multi-material non-food packagin...  9.228319e+08   2.056627\n",
      "9                     Electricity, coal (life cycle)  3.717960e+05   0.000829\n"
     ]
    }
   ],
   "source": [
    "# LCA Hotspots\n",
    "# Impact per provider for category 0\n",
    "CB = C @ B                                  # (n_impacts × n_providers)\n",
    "contrib = s * CB[2, :]                      # contribution per provider\n",
    "total_phi0 = contrib.sum()\n",
    "\n",
    "# Use provider names from A_index_df\n",
    "df = pd.DataFrame({\n",
    "    \"provider_name\": A_index_df[\"provider name\"].values,  # adjust column name if needed\n",
    "    \"contribution\": contrib\n",
    "})\n",
    "df[\"share_%\"] = 100 * df[\"contribution\"] / total_phi0\n",
    "\n",
    "# Top 10 contributors (by signed value)\n",
    "top10 = df.sort_values(\"contribution\", ascending=False).head(10)\n",
    "print(\"\\nTop 10 contributors to phi[0]:\")\n",
    "print(top10.reset_index(drop=True))\n",
    "\n",
    "# If you want absolute-value ranking (credits vs burdens):\n",
    "top10_abs = df.iloc[df[\"contribution\"].abs().sort_values(ascending=False).index].head(10)\n",
    "print(\"\\nTop 10 contributors to phi[x] (absolute):\")\n",
    "print(top10_abs.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d868fb5-8ae8-4ae9-9bb0-846a2942a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variable operating cost: $443.28 Billion\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ekey = lambda s: str(s).strip().casefold()\n",
    "\n",
    "# normalize and build index map\n",
    "A_index_df = A_index_df.copy()\n",
    "A_index_df[\"prov_key\"] = A_index_df[\"provider name\"].map(ekey)\n",
    "idx_map = A_index_df.groupby(\"prov_key\")[\"index\"].apply(lambda s: list(map(int, s))).to_dict()\n",
    "\n",
    "# foreground → column indices\n",
    "A_foreground_df = A_foreground_df.copy()\n",
    "A_foreground_df[\"prov_key\"] = A_foreground_df[\"provider name\"].map(ekey)\n",
    "fg_cols_by_name = {}\n",
    "for name, key in zip(A_foreground_df[\"provider name\"], A_foreground_df[\"prov_key\"]):\n",
    "    cols = idx_map.get(key, [])\n",
    "    if cols:\n",
    "        fg_cols_by_name.setdefault(name, []).extend(int(c) for c in cols)\n",
    "\n",
    "# financial rows → row indices with per-unit cost\n",
    "financial_df = financial_df.copy()\n",
    "if pd.api.types.is_numeric_dtype(financial_df[\"LCI Column Index\"]):\n",
    "    financial_df[\"row_index\"] = financial_df[\"LCI Column Index\"].astype(int)\n",
    "else:\n",
    "    financial_df[\"row_key\"] = financial_df[\"LCI Column Index\"].map(ekey)\n",
    "    financial_df[\"row_index\"] = financial_df[\"row_key\"].map(lambda k: idx_map.get(k, [None])[0])\n",
    "\n",
    "financial_df[\"Value\"] = pd.to_numeric(financial_df[\"Value\"], errors=\"coerce\")\n",
    "fin_rows = financial_df.dropna(subset=[\"row_index\", \"Value\"]).copy()\n",
    "fin_rows[\"row_index\"] = fin_rows[\"row_index\"].astype(int)\n",
    "cost_by_row = fin_rows.groupby(\"row_index\")[\"Value\"].sum().to_dict()\n",
    "\n",
    "# ABS versions of A and s\n",
    "A_abs = np.abs(A)\n",
    "s_abs = np.abs(s)\n",
    "\n",
    "# compute cost per foreground process\n",
    "cost_by_foreground = {}\n",
    "for fg_name, col_list in fg_cols_by_name.items():\n",
    "    total_cost_fg = 0.0\n",
    "    for c in set(col_list):\n",
    "        sc = float(s_abs[c]) if np.isfinite(s_abs[c]) else 0.0\n",
    "        if sc == 0.0:\n",
    "            continue\n",
    "        for r, unit_cost in cost_by_row.items():\n",
    "            a_val = float(A_abs[r, c]) if np.isfinite(A_abs[r, c]) else 0.0\n",
    "            if a_val != 0.0 and unit_cost is not None:\n",
    "                total_cost_fg += a_val * sc * float(unit_cost)\n",
    "    cost_by_foreground[fg_name] = total_cost_fg\n",
    "\n",
    "total_variable_cost = sum(cost_by_foreground.values())\n",
    "# print formatted in million dollars\n",
    "print(\"Total variable operating cost:\",\n",
    "      f\"${total_variable_cost/1e9:,.2f} Billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83bd7133-48eb-4a2b-be86-ac5781fe73f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most expensive foreground processes:\n",
      "Hydrocarbon feedstock production for use in steam cracker: $141.02 Billion\n",
      "Xylenes, mixed, produced from naphtha, at plant, kg: $46.36 Billion\n",
      "PET Drinking Bottles: $31.35 Billion\n",
      "PP Other Food Rigid: $24.26 Billion\n",
      "Ethanol, denatured, forest residues, thermochem: $17.09 Billion\n",
      "Ethylene glycol, fossil: $11.35 Billion\n",
      "Polyethylene, low-density, LDPE, virgin resin, food grade: $10.74 Billion\n",
      "HDPE Non-food Bottles: $9.65 Billion\n",
      "Purified terephthalic acid, PTA, fossil: $9.58 Billion\n",
      "Propylene production, fossil, steam cracker: $7.96 Billion\n"
     ]
    }
   ],
   "source": [
    "# sort the processes by cost (descending)\n",
    "top10 = sorted(cost_by_foreground.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 most expensive foreground processes:\")\n",
    "for name, val in top10:\n",
    "    print(f\"{name}: ${val/1e9:,.2f} Billion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed4ae4bc-7a13-481a-9c62-d5c908bd05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "biodiversity_breakdown_df = pd.read_csv(\"data/biodiversity_breakdown.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ea558-8bf7-46ee-b836-312d5fffc2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for robust matching\n",
    "norm = lambda x: str(x).strip().casefold()\n",
    "\n",
    "# copies + normalized keys\n",
    "B_index_df = B_index_df.copy()\n",
    "biodiversity_breakdown_df = biodiversity_breakdown_df.copy()\n",
    "\n",
    "B_index_df[\"flow_key\"] = B_index_df[\"flow name\"].map(norm)\n",
    "biodiversity_breakdown_df[\"flow_key\"] = biodiversity_breakdown_df[\"flow name\"].map(norm)\n",
    "\n",
    "# keep only flows present in both tables\n",
    "bd_subset = biodiversity_breakdown_df[biodiversity_breakdown_df[\"flow_key\"].isin(B_index_df[\"flow_key\"])].copy()\n",
    "\n",
    "# get the flow indices (row indices in B, column indices in C) in the SAME order as bd_subset\n",
    "flow_idx_map = B_index_df.set_index(\"flow_key\")[\"index\"].astype(int)\n",
    "flow_indices = bd_subset[\"flow_key\"].map(flow_idx_map).astype(int).to_numpy()\n",
    "\n",
    "# pull CFs (ensure numeric)\n",
    "micro_vals = pd.to_numeric(bd_subset[\"microplastic\"], errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "macro_vals = pd.to_numeric(bd_subset[\"macroplastic\"], errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "\n",
    "# make two C variants and overwrite the second-row CFs at the matched flows\n",
    "C_micro = C.copy()\n",
    "C_macro = C.copy()\n",
    "\n",
    "row_phi1 = 1  # second row (0-based)\n",
    "C_micro[row_phi1, flow_indices] = micro_vals\n",
    "C_macro[row_phi1, flow_indices] = macro_vals\n",
    "\n",
    "# compute phi using phi = C @ (B @ s)\n",
    "inventory = B @ s  # shape (n_flows,)\n",
    "phi_micro = C_micro @ inventory\n",
    "phi_macro = C_macro @ inventory\n",
    "\n",
    "# extract the second component (phi[1]) for each case\n",
    "phi1_micro = float(phi_micro[row_phi1])\n",
    "phi1_macro = float(phi_macro[row_phi1])\n",
    "\n",
    "print(f\"phi[1] due to microplastics: {phi1_micro}\")\n",
    "print(f\"phi[1] due to macroplastics: {phi1_macro}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
